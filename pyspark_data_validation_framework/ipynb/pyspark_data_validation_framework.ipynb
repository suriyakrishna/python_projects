{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/pyspark_data_validation_framework.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/pyspark_data_validation_framework.py\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class data_frame_data_validation:\n",
    "\n",
    "    def __init__(self, df1, df2, primary_keys, list_of_columns=None, num_records_to_validate=5):\n",
    "        if(isinstance(df1, DataFrame) and isinstance(df1, DataFrame)):\n",
    "            self.df1 = df1\n",
    "            self.df2 = df2\n",
    "            self.intersecting_columns = list(set(df1.columns).intersection(df2.columns))\n",
    "            if(len(self.intersecting_columns) == 0):\n",
    "                print(\"Provided Data Frame doesn't have same schema. Exiting..\")\n",
    "                sys.exit(1)\n",
    "            self.num_records_to_validate = list(range(1,num_records_to_validate+1))\n",
    "            if(isinstance(primary_keys, str)):\n",
    "                self.primary_keys = list(map(lambda x: x.strip(), primary_keys.split(\",\")))\n",
    "            else:\n",
    "                self.primary_keys = primary_keys\n",
    "            if(list_of_columns != None):\n",
    "                if(isinstance(list_of_columns, str)):\n",
    "                    self.list_of_columns = list(map(lambda x: x.strip(), list_of_columns.split(\",\")))\n",
    "                else:\n",
    "                    self.list_of_columns = list_of_columns\n",
    "            else:\n",
    "                self.list_of_columns = df1.columns\n",
    "        else:\n",
    "            print(\"Parameter type violation. Exiting\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def create_window(self):\n",
    "        window = Window.orderBy(self.primary_keys)\n",
    "        return window\n",
    "\n",
    "    def data_preparation(self, df, no_of_rows_to_validate):\n",
    "        w = self.create_window()\n",
    "        output = df.withColumn(\"row_num\", row_number().over(w)).filter(col(\"row_num\").isin(no_of_rows_to_validate)).orderBy(col(\"row_num\")).collect()\n",
    "        return list(map(lambda x: x.asDict(), output))\n",
    "\n",
    "    def do_validation(self, output_file_path=None):\n",
    "        source_list_dict = self.data_preparation(self.df1, self.num_records_to_validate)\n",
    "        dest_list_dict = self.data_preparation(self.df2, self.num_records_to_validate)\n",
    "        a = []\n",
    "        b = []\n",
    "        r = []\n",
    "        for i in range(len(self.num_records_to_validate)):\n",
    "            A = []\n",
    "            B = []\n",
    "            R = []\n",
    "            for column in self.list_of_columns:\n",
    "                df1_value = str(source_list_dict[i][column])\n",
    "                df2_value = str(dest_list_dict[i][column])\n",
    "                A.append(df1_value)\n",
    "                B.append(df2_value)\n",
    "                R.append(str(df1_value == df2_value))\n",
    "            a.append(\"~\".join(A))\n",
    "            b.append(\"~\".join(B))\n",
    "            r.append(\"~\".join(R))\n",
    "        c = \"~\".join(self.list_of_columns)\n",
    "        a = \"\\n\".join(a)\n",
    "        b = \"\\n\".join(b)\n",
    "        r = \"\\n\".join(r)\n",
    "\n",
    "        print(\"SOURCE DATA\\n\")\n",
    "        print(c)\n",
    "        print(a)\n",
    "        print(\"\\nDESTINATION DATA\\n\")\n",
    "        print(c)\n",
    "        print(b)\n",
    "        print(\"\\nRESULT\\n\")\n",
    "        print(c)\n",
    "        print(r)\n",
    "        \n",
    "        if(output_file_path != None):\n",
    "            current_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "            output_file = os.path.join(output_file_path, \"data_validation_output_{}.csv\".format(current_timestamp))\n",
    "            with open(output_file, 'w') as write_output:\n",
    "                write_output.write(\"SOURCE DATA\\n\")\n",
    "                write_output.writelines(c+\"\\n\")\n",
    "                write_output.writelines(a+\"\\n\")\n",
    "                write_output.write(\"\\nDESTINATION DATA\\n\")\n",
    "                write_output.writelines(c+\"\\n\")\n",
    "                write_output.writelines(b+\"\\n\")\n",
    "                write_output.write(\"\\nRESULT\\n\")\n",
    "                write_output.writelines(c+\"\\n\")\n",
    "                write_output.writelines(r+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark - Data Validation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "source_file_path = os.path.join(\"../inputs\", \"employee_source.csv\")\n",
    "destination_file_path = os.path.join(\"../inputs\", \"employee_destination.csv\")\n",
    "header = \"true\"\n",
    "infer_schema = \"true\"\n",
    "\n",
    "# Creating Two DataFrames which has same schema\n",
    "\n",
    "source_df = spark.read.option(\"infer\", infer_schema).option(\"header\", \"true\").csv(source_file_path)\n",
    "destination_df = spark.read.option(\"infer\", infer_schema).option(\"header\", \"true\").csv(destination_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Manager: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scehma of DataFrame 1\n",
    "source_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Manager: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scehma of DataFrame 2\n",
    "destination_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+------+\n",
      "|Name         |Department|Manager    |Salary|\n",
      "+-------------+----------+-----------+------+\n",
      "|Robin Hood   |Bar       |null       |200   |\n",
      "|Arsene Wenger|Bar       |Friar Tuck |50    |\n",
      "|Friar Tuck   |Foo       |Robin Hood |100   |\n",
      "|Little John  |Foo       |Robin Hood |100   |\n",
      "|Sam Allardyce|Bar       |null       |250   |\n",
      "|Dimi Berbatov|Foo       |Little John|50    |\n",
      "+-------------+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample Data in DataFrame 1\n",
    "source_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+------+\n",
      "|Name         |Department|Manager    |Salary|\n",
      "+-------------+----------+-----------+------+\n",
      "|Robin Hood   |null      |null       |200   |\n",
      "|Arsene Wenger|Bar       |Friar Tuck |50    |\n",
      "|Friar Tuck   |Foo       |Robin Hood |100   |\n",
      "|Little John  |Foo       |Robin Hood |100   |\n",
      "|Sam Allardyce|null      |null       |250   |\n",
      "|Dimi Berbatov|Foo       |Little John|50    |\n",
      "+-------------+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scehma of DataFrame 2\n",
    "destination_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data validation module\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "from pyspark_data_validation_framework import data_frame_data_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame Data Validation Object\n",
    "# For the class the first and second argument are source and destination DataFrames respectively.\n",
    "# Third Argument is the primary_keys which are mandatory inorder to select records based on primary keys.\n",
    "# Fourth Argument is the num_records_to_validate\n",
    "data_validation = data_frame_data_validation(source_df, destination_df, primary_keys=[\"Name\"], num_records_to_validate=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE DATA\n",
      "\n",
      "Name~Department~Manager~Salary\n",
      "Arsene Wenger~Bar~Friar Tuck~50\n",
      "Dimi Berbatov~Foo~Little John~50\n",
      "Friar Tuck~Foo~Robin Hood~100\n",
      "Little John~Foo~Robin Hood~100\n",
      "Robin Hood~Bar~None~200\n",
      "Sam Allardyce~Bar~None~250\n",
      "\n",
      "DESTINATION DATA\n",
      "\n",
      "Name~Department~Manager~Salary\n",
      "Arsene Wenger~Bar~Friar Tuck~50\n",
      "Dimi Berbatov~Foo~Little John~50\n",
      "Friar Tuck~Foo~Robin Hood~100\n",
      "Little John~Foo~Robin Hood~100\n",
      "Robin Hood~None~None~200\n",
      "Sam Allardyce~None~None~250\n",
      "\n",
      "RESULT\n",
      "\n",
      "Name~Department~Manager~Salary\n",
      "True~True~True~True\n",
      "True~True~True~True\n",
      "True~True~True~True\n",
      "True~True~True~True\n",
      "True~False~True~True\n",
      "True~False~True~True\n"
     ]
    }
   ],
   "source": [
    "# Calling Data Validation Method. This will print the output in the console\n",
    "# If output_file_path argument is not provided the output will be printed only in console and it will not create output file\n",
    "data_validation.do_validation(\"../outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame with different schema to test for the failure case\n",
    "column_name = \"timestamp,userid\"\n",
    "values = \"2018-01-01T11:00:00Z u1, 2018-01-01T12:00:00Z u1, 2018-01-01T11:00:00Z u2, 2018-01-02T11:00:00Z u2, 2018-01-01T12:15:00Z u1,  2018-01-01T12:30:00Z u1,  2018-01-01T12:45:00Z u1,  2018-01-01T13:14:00Z u1,  2018-01-01T13:35:00Z u1, 2018-01-01T13:55:00Z u1, 2018-01-01T14:20:00Z u1, 2018-01-01T14:45:00Z u1, 2018-01-01T15:05:00Z u1\"\n",
    "list_row = list(map(lambda x: tuple(x.strip().split(\" \")),values.split(\",\")))\n",
    "df = spark.sparkContext.parallelize(list_row).toDF(column_name.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided Data Frame doesn't have same schema. Exiting..\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kishan\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Comparing Two Data Frames of different schema will fails the applications\n",
    "data_validation1 = data_frame_data_validation(df, destination_df, primary_keys=\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure case for empty primary_keys\n",
    "data_validation1 = data_frame_data_validation(source_df, destination_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
